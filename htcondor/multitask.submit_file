####################
#
# Example Job for HTCondor
#
####################

#---------------------------------------------
# Name your batch so it's easy to distinguish in the q.
JobBatchName = "Multitask for metaphor generation"

# --------------------------------------------
# Executable and its arguments

model_name = uer/gpt2-chinese-cluecorpussmall
cmc_path = $ENV(HOME)/cmc/CMC/old_version/zh_train.tsv
clc_path = /vol/research/lyc/cmc/CLC_processed/processed_sentences.txt
simile_path = $ENV(HOME)/cmc/Chinese-Simile-Recognition-Dataset
max_length = 256
epochs = 1
save_path = /vol/research/lyc/cmc/checkpoint/multitask
simile_script_path = $ENV(HOME)/cmc/model/chinese_simile_dataset.py
cache_dir = /mnt/fast/nobackup/users/yl02706/.cahce/huggingface/dataset

executable    = $ENV(HOME)/.conda/envs/lyc/bin/python
arguments     = $ENV(HOME)/cmc/model/multitask.py $(model_name) $(cmc_path) $(clc_path) $(simile_path) $(max_length) $(epochs) $(save_path) $(simile_script_path) $(cache_dir)

# ---------------------------------------------------
# Universe (vanilla, docker)
universe         = docker
docker_image     = nvidia/cuda:10.2-cudnn7-runtime-ubuntu16.04

# -------------------------------------------------
# Event, out and error logs
log    = Generation.Multitask.c$(cluster).p$(process).log
output = Generation.Multitask.c$(cluster).p$(process).out
error  = Generation.Multitask.c$(cluster).p$(process).error

# -----------------------------------
# File Transfer, Input, Output
should_transfer_files = YES

# Make certain project spaces available in container
# environment = "mount=$ENV(HOME)"
environment = "mount=/vol/research/nlg,/vol/research/lyc_d,/vol/research/lyc"

# -------------------------------------
# Requirements for the Job (Requirements are explained in further detail in example09.submit_file)
# NOTE: HasStornext is not valid on orca.
requirements = (CUDAGlobalMemoryMb > 15000) && (CUDAGlobalMemoryMb <  25000) && \
               (HasStornext) && \
			   (CUDACapability > 2.0)

# --------------------------------------
# Resources
request_GPUs     = 1
# this needs to be specified for the AI@Surrey cluster if requesting a GPU
+GPUMem          = 10000
request_CPUs     = 4
request_memory   = 8G

#This job will complete in less than 1 hour
+JobRunTime = 240

#This job can checkpoint
+CanCheckpoint = true

# -----------------------------------
# Queue commands
queue
